
# Around "winning" VRF contests

As in Ouroboros Praos, we have a node with VRF key $V = v G$ produce a block, or "win" in another way, whenever
$$ \texttt{VRF}_v(\texttt{slot_seed}) < c \, \texttt{stake}_v / \texttt{total_stake} $$
for some $c$, not necessarily constant.  We note that a single VRF may produce an almost unlimited amounts of data for such comparisons, or other uses, by seeding a stream cipher as a CSPRNG.

## Proofs of not "winning" recent VRF contests 

As previously discussed [1,2], we should defend against "speed up attacks" by limiting how quickly staked but inactive nodes can impact the block production rate.  We note this cannot protect against biased randomness because nodes can always ensure their block lands as an uncle block, and uncles cannot influence the randomness without creating worse attacks.  

We now alter the above VRF "winner" formula to facilite these "non-winner" proofs.  Intuitively, we produce a time until our next block from the VRF evaluation, instead of evaluating our VRF on every slot.

Assuming $V$ has a block at height $i_0$, we shall define $s_{i_0}$ and sample a delay from $\texttt{VRF}_v(s_{i_0})$ to pick $V$'s next block height $i_{+1} = i_0 + \textrm{delay}$.  
$$ (d,\ldots) := \texttt{VRF}_v(s_{i_0})  
   \quad\textrm{and}\quad
   i_{+1} := i_0 + f(d) $$
where $f(d)$ is a function determined by the stake and desired block rate, like $\left\lceil d \, c \, \textrm{stake}_v / \textrm{total_stake} \right\rceil$, or more likely sampling from an exponential distribution with $\lambda = c \, \textrm{stake}_v / \textrm{total_stake}$. 

We must verify this VRF at height $i_{+1}$ however, so $s_{i_0}$ must be available then.  We have long delays for poorly staked nodes, so $(i_0,s_{i_0})$ should be created and written to $V$'s account by their previous block at height $i_{-1}$, and similarly $s_{i_{+1}}$ must be defined by $s_{i_0}$.  

We might for example define $s_{i_0}$ to be some collaborative random value created before $i_{-1}$, so
$$ (d,s'_{i_0+100}) := \texttt{VRF}_v(s_{i_0}) 
   \quad\textrm{and}\quad
   s_{i_{+1}} := s'_{i_0} \oplus s'_{i_0-1} \oplus \cdots $$
We fear defining $s'_{i_0}$ at height $i_0-100$ like this creates problems at block height $i_{+1}$ whenever $V$ skips their block at height $i_0$:  If $V$ skips $i_0$ then they have not written $s_{i_{+1}}$ into their account, and they might not know $s'_{i_0}$ defined at block height $i_0-100$ from which it gets defined.  Worse, any recently assigned validators might not know these blocks either. 

In case $V$ misses $i_0$ then we permit $V$ to win with a simpler VRF winning condition, like
$$ \texttt{VRF}_v(s'_{i-100}) < c/10 \, \texttt{stake}_v / \texttt{total_stake} $$
We require however that $V$ prove they already missed their randomness by also providing
$$ (d,\ldots) := \texttt{VRF}_v(s_{i_{-1}}) $$
where their account contains $(i_{-1},s_{i_{-1}})$, along with $i > i_{-1} + f(d)$.  We could further penalize skipping blocks without adding slashing by either adding some constant number of penalty blocks to this lower bound on $i$, or else decreasing the block rate in $f$.


## Adjusting the block rate 

We shall adjust the block rate determined by our paramater $c$ inside $f$.  We could even adjust the block rate for each VRF mode separately.  There is perhaps a trade off between slowing small block producers joining in mass, as they might be corrupted, vs resisting censorship by permitting them to produce at the same rate, meaning we keep both block rates identical.  As a compromise, we might dynamically make the block rate for our second non-skipping Praos-style VRF lower when more people use it, likely only temporarily.  

## VDF Integration

We want an unbiasable randomness source as well, so we shall xor randomness from all recent block in both the chain and uncles, and push the result through a long running VDF, ala Ethereum.  We may have fewer inputs than Ethereum since we only recieve randomness one block at a time and they can likely take in many in parallel due to using hash chains.  Also, our VDF delay must be $A_{\texttt{max}} \cdot \texttt{block_rate} \cdot \texttt{blocks}$ where each block contributes one randomness source.

We include uncles only because including them improves confidence in our randomness.  In fact, attackers could just exclude uncles, so they do not improve our randomness per se.  Also, if you use GHOST then including randomness from uncles also weakens attacker chains, and makes them more detectable, by forcing them to exclude many uncles.

There are several VDF schemes but [both likely candidates](https://crypto.stanford.edu/~dabo/pubs/papers/VDFsurvey.pdf) use groups of unknown order.  We require the ability to bind the VDF proofs to some validator's session key, but such tricks sound relatively generic, like repalcing some PRFs in the proof with VRFs.

There are also two favored implementations for groups of unknown order: 
 - Ethereum v2 wants an RSA composite with unknown factors, constructed with a large trusted setup.  ASICs can massively accellerate such VDFs, so Ethereum wants to develop and gift an open sources ASIC.
 - Chia network wants a class group of an imaginary quaratic order, although [their competition looks under funded](https://github.com/Chia-Network/vdf-competition).  Such VDFs sounds potentially ASIC resistant, but ASIC resistnace is inherently ephemeral.

Assuming anyone implements and runs the trusted setup, we suggest an optimal approach to be starting with RSA until Ethereum makes tangible progress on their ASIC, and then switch to class groups before the ASIC is completed.  We might switch back to RSA later after Ethereum's ASIC becomes "commodity".  We must however be careful about telling validators that they require special hardware, which suggests we should lag behind Ethereum's ASIC project.  

## Alternatives

We enumerate several alternative schemes rejected in designing the above scheme.

### Old randomness avaiability 

There are numerous alternatives for old randomness not being available in this same basic scheme: 

*Frist*, we could fix up the scheme outlined initially by making $V$ purchase proofs of $s'_{i_0}$ etc from archival nodes.  This sounds realistic, but may sometimes force $V$ into unstaking and restaking. 

*Second*, we might divorce $s_{i_0}$ entirely from recent collaborative randomness ala
$(d,s_{V,i_1}) := \texttt{VRF}_v(s_{V,i_0})$.  We'd still begin $V$'s skip sequence with collaborative randomness, but this lives long in the past, likely exceeding the unbonding period, so the VRF key $V$ could repeatedly reregister to "bruit force" themselves recieving more or sooner blocks, making this highly problematic.

*Third*, we might permit a "stretched" VRF route with which $V$ may continue their skip sequence using their old randomness despite missing a block.  This sounds workable, but it introduces bias because we permit $V$ to base their next block on very old randomness. 

*Fourth*, we minimize this bias by making $V$ first replace their randomness on chain:  First $V$ submits a "skip apology" transaction that proves they missed the block permitted by the randomness $s_{i_0}$ saved in their account.  Second if $j$ is the block height of this "skip apology" transaction, then the randomness stored in $V$'s account is overwritten by the randomness created at block height $j+100$, so their record containing $(i_0,s_{i_0})$ gets replaced with $(j,s_j)$.  These "skip apology" transactions incur costs to $V$ however, especially since they force content into the later block $j+100$.  We might begin skip sequences this way too.

*Fifth*, we could require an simpler VRF win to begin skip sequences, so maybe
$$ \texttt{VRF}_v(s'_{i-100}) < c/10 \, \texttt{stake}_v / \texttt{total_stake} $$
In which case, we could permit free skip sequences restarts whenever this VRF win occurs, so nodes can restart without apology transactions, but now they can only prove they skipped less than say 10% of their blocks in expectation. 

### "Non-winner" proof schemes

There are also several unrelated alternative approaches to these "non-winner" proofs:

#### Alternative 1.  Use hash chaining as a VRF

If you use a long hash chain as a VRF like RANDAO, then you must reveal all those intermediate values periodically anyways, but checking them is faster than checking a VRF based on public key primitives.  Yet, this creates all RANDAO's control over last reveal problems.  

#### Alternative 2.  Use signature aggregation

We can batch verify BLS signatures used as VRFs very efficiently here because $V$ is the public key for all VRFs evaluated by this one node.  We must however provide all the intermediate BLS signatures as VRF results, so this only provides faster batch verification, not actual aggregation.

We might try SNARG/SNARKs, or range proofs, that no recent VRF wins occurred, even with Ouroboros Praos like formula, along with actual aggregation, but these requires heavy computation, and considerable code complexity.

#### Alternative 3.  Layers of VRFs

Intuitively, we have layers of VRFs that determine "winning" for different "time" scales, like week, day, hour, etc., but more likely 1, 4, 16, 64, 256, 1024, 4096, etc. blocks.  You must "win" all layers to create a block.  We define the VRF at layer $l$ by 
$$ \texttt{VRF}_v(s_i ++ l) < c_l \, \texttt{stake}_v / \texttt{total_stake} $$
but note the $c_l$ should all be equal assuming ratios between adjacent layer durations agree.  We envision the longest VRF time scale being shorter than the unbonding period, but perhaps comparable duration.  If using BLS, we can aggregate these VRFs efficiently because the public keys match.

We know block heights, so VRF evaluation points should correspond to block heights, and thus layer durations should be block counts.  We must however store the collaborative randomness for relevant VRF evaluation points, but winning in one layer does not guarantee winning any tranche on the sublayer, so we must ultimately store all block heights within the unbonding period.  If the node's stake is reasonable, then they should've produced a recent block, and thus their proof only refers to relatively recent blocks.  Any poorly staked nodes who miss blocks might need to purchase proofs of old randomness from archival nodes though.

TODO:  What node state was kept here?

[1] https://forum.parity.io/t/inter-chain-message-passing-research-meeting/153
[2] https://medium.com/web3foundation/w3f-research-workshop-outcomes-6320ae328222
